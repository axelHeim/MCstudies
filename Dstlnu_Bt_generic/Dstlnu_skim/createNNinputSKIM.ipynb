{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time at start = 2021-09-24 10:49:25.378569\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "print(\"time at start =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTCondorRun: -f\n"
     ]
    }
   ],
   "source": [
    "HTCondorRun = str(sys.argv[1])\n",
    "print(\"HTCondorRun:\",HTCondorRun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = False\n",
    "tmp_data = False\n",
    "take_subset = True\n",
    "subset_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs_path = \"/nfs/dust/belle2/user/axelheim/MC_studies/Dstlnu_Bt_generic/\"\n",
    "\n",
    "data_subdir = \"Dstlnu_skim_BsBtag_separation_{}_evts/\".format(subset_size)   \n",
    "root_subdir = \"axheim_DstlnuSKIM_run1/\"   \n",
    "\n",
    "root_path = nfs_path + \"Dstlnu_skim/\" + root_subdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = \"merged_\"\n",
    "if tmp_data:\n",
    "    merged += \"tmp_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileY4S = uproot.open(root_path + merged + \"DXtagDstl.root:variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/dust/belle2/user/axelheim/MC_studies/Dstlnu_Bt_generic/Dstlnu_skim/axheim_DstlnuSKIM_run1/merged_gammas.root:variables\n"
     ]
    }
   ],
   "source": [
    "names = [\"gammas\",\"electrons\",\"pions\",\"kaons\",\"muons\"]\n",
    "dfs = []\n",
    "for name in names:\n",
    "    filename = root_path + merged + \"{}.root:variables\".format(name)\n",
    "    print(filename)\n",
    "    tmpFileFSPs = uproot.open(filename)\n",
    "    df_tmp = tmpFileFSPs.arrays(library=\"pd\")\n",
    "    dfs.append(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FSPs = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y4S = fileY4S.arrays(library=\"pd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_FSPs.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete FSPs for which no Y4S file entry was found\n",
    "#df_FSPs = df_FSPs[df_FSPs['__event__'].isin(df_Y4S[\"__event__\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_FSPs.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take a sample if used in notebook for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_evt_nums = df_FSPs['__event__'].unique()\n",
    "all_evt_nums.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_evt_nums = np.random.choice(all_evt_nums, size=subset_size)\n",
    "sample_evt_nums.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if take_subset:\n",
    "    df_FSPssample = df_FSPs[df_FSPs['__event__'].isin(sample_evt_nums)]\n",
    "    #df_Y4S=df_Y4Ssample\n",
    "    df_FSPs=df_FSPssample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"df_FSPs.shape[0]:\",df_FSPs.shape[0])\n",
    "print(\"numEvents:\",df_FSPs['__event__'].unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete particles which occur more than ones based on uniqueParticleIdentifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupsFSPs_uniqParID = pd.DataFrame({'count' : df_FSPs.groupby( [\"__event__\",\"uniqueParticleIdentifier\"] ).size()}).reset_index()\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#    print(groupsFSPs_uniqParID.sort_values(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"df_FSPs.shape[0]:\",df_FSPs.shape[0])\n",
    "print(\"groupsFSPs_uniqParID.shape[0]:\",groupsFSPs_uniqParID.shape[0])\n",
    "print(\"df_Y4S.shape[0]:\",df_Y4S.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete particles which occur more than ones (keep first) and if possible keep the one with basf2_used==1\n",
    "print(\"df_FSPs[basf2_used].value_counts():\",df_FSPs[\"basf2_used\"].value_counts())\n",
    "df_FSPs = df_FSPs.sort_values(\"basf2_used\",ascending=False).drop_duplicates(subset=(\"__event__\",\"uniqueParticleIdentifier\"), keep='first')\n",
    "print(\"df_FSPs[basf2_used].value_counts():\",df_FSPs[\"basf2_used\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_FSPs.shape[0]:\",df_FSPs.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if take_subset:\n",
    "    df_FSPs.to_csv(root_path + \"df_FSPs_sample_{}_evts.csv\".format(subset_size))\n",
    "    #df_Y4S.to_csv(root_path + \"df_Y4S_sample__evts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "if load:\n",
    "    df_FSPs = pd.read_csv(root_path + \"df_FSPs_sample_{}_evts.csv\".format(subset_size))\n",
    "    #df_Y4S = pd.read_csv(root_path + \"df_Y4S_sample10evts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print one event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FSPs[\"__event__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FSPs[df_FSPs[\"__event__\"] == 6620998].sort_values('mcPDG')[['mcPDG','genMothPDG_0', 'mcMother0_uniqParID', 'genMotherID_0',\n",
    "'genMothPDG_1', 'mcMother1_uniqParID', 'genMotherID_1',\n",
    "'genMothPDG_2', 'mcMother2_uniqParID', 'genMotherID_2',\n",
    "'genMothPDG_3', 'mcMother3_uniqParID', 'genMotherID_3',\n",
    "'genMothPDG_4', 'mcMother4_uniqParID', 'genMotherID_4',\n",
    "'genMothPDG_5', 'mcMother5_uniqParID', 'genMotherID_5',\n",
    "'genMothPDG_6', 'mcMother6_uniqParID', 'genMotherID_6',\n",
    "'genMothPDG_7', 'mcMother7_uniqParID', 'genMotherID_7',\n",
    "'genMothPDG_8', 'mcMother8_uniqParID', 'genMotherID_8',\n",
    "'genMothPDG_9', 'mcMother9_uniqParID', 'genMotherID_9']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check if category combinations make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupsAllFSPs = pd.DataFrame({'count' : df_FSPs.groupby( [\"basf2_used\",\"basf2_Bsig\",\"basf2_X\"] ).size()}).reset_index()\n",
    "groupsAllFSPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add B_ID col with extra info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create col with the particles mother B's uniqueParticleIdentifier\n",
    "def B_ID(s):\n",
    "    label = 0\n",
    "    for i in range(10):\n",
    "        mcMotheri_uniqParID = \"mcMother{}_uniqParID\".format(i)\n",
    "        if ((s[mcMotheri_uniqParID]) == 83886082.0):\n",
    "            label = 83886082   \n",
    "        elif ((s[mcMotheri_uniqParID]) == 83886081.0):\n",
    "            label = 83886081   \n",
    "    return label\n",
    "df_FSPs['B_ID'] = df_FSPs.apply(B_ID, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create col with info which i-th daughter of B the particle is (0=daughter,1=granddaughter,.. -1=no B daughter)\n",
    "#def B_iDaughter(s):\n",
    "#    label = -1\n",
    "#    for i in range(10):\n",
    "#        mcMotheri_uniqParID = \"mcMother{}_uniqParID\".format(i)\n",
    "#        if ((s[mcMotheri_uniqParID] == 83886082.0) or (s[mcMotheri_uniqParID] == 83886081.0)):\n",
    "#            label = i   \n",
    "#            return label\n",
    "#    return label\n",
    "#df_FSPs['B_iDaughter'] = df_FSPs.apply(B_iDaughter, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_FSPs[\"mcPDG_abs\"] = df_FSPs[\"mcPDG\"].abs()\n",
    "#mcPDG_BID_combis = pd.DataFrame({'count' : df_FSPs.groupby( [\"mcPDG_abs\",\"B_ID\",\"B_iDaughter\"] ).size()}).reset_index()\n",
    "\n",
    "\n",
    "#printDF = mcPDG_BID_combis[mcPDG_BID_combis[\"B_iDaughter\"]==0].sort_values('B_ID',ascending=False)\n",
    "\n",
    "#pd.set_option('display.float_format', lambda x: '%.0f' % x)\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#    print(printDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this shows that Hc is sometimes combined from both B's which is of course wrong\n",
    "#groupsAllFSPs = pd.DataFrame({'count' : df_FSPs.groupby( [\"__event__\",\"B_ID\",\"Hc\",\"basf2_used\",\"basf2_Bsig\",\"basf2_X\"] ).size()}).reset_index()\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#    print(groupsAllFSPs.sort_values(\"Hc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save dataframes on NFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HTCondorRun == \"isHTCondorRun\":\n",
    "    df_FSPs.to_csv(root_path + \"df_FSPs_preProcessed_{}_evts.csv\".format(subset_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_FSPs = pd.read_csv(root_path + \"df_FSPs_preProcessed.csv\")\n",
    "#event_Bs = pd.read_csv(root_path + \"event_Bs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FSPs_final = df_FSPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start of NN data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFSPs = pd.DataFrame({'count' : df_FSPs_final.groupby( [\"__event__\"] ).size()}).reset_index()\n",
    "\n",
    "minFSPs = numFSPs[\"count\"].min()\n",
    "maxFSPs = numFSPs[\"count\"].max()\n",
    "print(\"minFSPs:\",minFSPs)\n",
    "print(\"maxFSPs:\",maxFSPs,'\\n')\n",
    "\n",
    "df_FSPs_final['numFSPs'] = df_FSPs_final.groupby('__event__')['__event__'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(nfs_path + \"data/\" + data_subdir + root_subdir)    \n",
    "if save_data:\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Will save data to:\", data_dir,'is', save_data ,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for num_FSPs_toData in range(minFSPs, maxFSPs+1):\n",
    "    df_num_subset = df_FSPs_final.copy()\n",
    "    df_num_subset = df_num_subset[df_num_subset['numFSPs'] == num_FSPs_toData]\n",
    "    \n",
    "        \n",
    "    numEvents = df_num_subset.__event__.nunique()\n",
    "    print(\"numEvents:\",numEvents)\n",
    "    print(\"num_FSPs_toData:\",num_FSPs_toData)  \n",
    "    if numEvents == 0:\n",
    "        print(\"skipped because empty \\n\")\n",
    "        continue\n",
    "\n",
    "    if numEvents < 10:\n",
    "        print(\"skipped because <10 events \\n\")\n",
    "        continue\n",
    "    \n",
    "    num_features = 4\n",
    "    leaves = np.zeros((numEvents, num_FSPs_toData,  num_features))  \n",
    "    SA_target =  np.zeros((numEvents, num_FSPs_toData))\n",
    "    global_tag = np.chararray((numEvents, num_FSPs_toData + 1), itemsize=30)\n",
    "    \n",
    "    event_list = df_num_subset[df_num_subset[\"numFSPs\"] == num_FSPs_toData][\"__event__\"].unique()\n",
    "    #print(\"len(event_list):\",len(event_list))\n",
    "    for i in range(numEvents):\n",
    "\n",
    "        event_iter = event_list[i]\n",
    "\n",
    "        global_tag_masterInfo = \"evt\" + str(event_iter)\n",
    "        global_tag[i,-1] = global_tag_masterInfo\n",
    "        #print(\"global_tag[i,-1]:\",global_tag[i,-1])\n",
    "        #print(\"i:\",i,\"event_iter:\",event_iter)\n",
    "\n",
    "        event_df = df_num_subset[df_num_subset.__event__ == event_iter]\n",
    "\n",
    "        for j in range(num_FSPs_toData):\n",
    "            #print(\"numParticle:\",j)\n",
    "            particle = event_df.iloc[j]\n",
    "\n",
    "            #print(particle[\"mcPDG\"],particle[\"px\"],particle[\"py\"],particle[\"pz\"],particle[\"E\"])\n",
    "            leaves[i,j,0] = particle[\"px\"]\n",
    "            leaves[i,j,1] = particle[\"py\"]\n",
    "            leaves[i,j,2] = particle[\"pz\"]\n",
    "            leaves[i,j,3] = particle[\"E\"]\n",
    "            \n",
    "            basf2_usage = \"basf2_NONE\"\n",
    "            if particle[\"basf2_Bsig\"] == 1.0:\n",
    "                basf2_usage = \"basf2_Bsig\"\n",
    "            elif particle[\"basf2_X\"] == 1.0:\n",
    "                basf2_usage = \"basf2_X\"\n",
    "            elif particle[\"basf2_used\"] == 0:\n",
    "                basf2_usage = \"basf2_bg\"\n",
    "\n",
    "            global_tag_Info = str((particle[\"mcPDG\"])) \n",
    "            global_tag_Info += \"_\" + basf2_usage\n",
    "            global_tag[i,j] = global_tag_Info\n",
    "\n",
    "            label = -10 # error code if assignment fails\n",
    "            B_tag_uniqID = 83886081.0\n",
    "            B_sig_uniqID = 83886082.0\n",
    "            if particle[\"B_ID\"] == B_tag_uniqID:\n",
    "                label = 1 # particle belongs to Btag (MC truth)\n",
    "            elif particle[\"B_ID\"] == B_sig_uniqID:\n",
    "                label = 2 # particle belongs to Bsig (MC truth)\n",
    "            elif particle[\"B_ID\"] == 0:\n",
    "                label = 0 # background\n",
    "            \n",
    "            \n",
    "            SA_target[i,j] = label\n",
    "            \n",
    "        del event_df\n",
    "        \n",
    "        \n",
    "    # shuffle the data    \n",
    "    for idx in np.arange(leaves.shape[0]):   # arange is like range but gives ndarray instead of list\n",
    "        perms = np.random.permutation(leaves.shape[1])\n",
    "\n",
    "        leaves[idx,:] = leaves[idx,perms]\n",
    "        SA_target[idx,:] = SA_target[idx,perms]\n",
    "        global_tag[idx,0:-1] = global_tag[idx,perms]\n",
    "        \n",
    "        \n",
    "         \n",
    "\n",
    "\n",
    "    #print(global_tag)\n",
    "    train_ratio = 0.75\n",
    "    validation_ratio = 0.15\n",
    "    test_ratio = 0.10\n",
    "\n",
    "    print(\"leaves.shape:\",leaves.shape)\n",
    "    print(\"SA_target.shape:\",SA_target.shape)\n",
    "    print(\"global_tag.shape:\",global_tag.shape)\n",
    "\n",
    "\n",
    "    print(\"leaves[0]:\",leaves[0])\n",
    "    print(\"SA_target[0]:\",SA_target[0])\n",
    "    print(\"global_tag[0]:\",global_tag[0])\n",
    "\n",
    "    x=leaves\n",
    "    y=SA_target\n",
    "    z=global_tag\n",
    "\n",
    "    x_train, x_test, y_train, y_test, z_train, z_test = train_test_split(x, y, z, test_size=1 - train_ratio, shuffle=False)\n",
    "    x_val, x_test, y_val, y_test, z_val, z_test = train_test_split(x_test, y_test, z_test, test_size=test_ratio/(test_ratio + validation_ratio), shuffle=False) \n",
    "\n",
    "    if save_data==True:\n",
    "        np.save(data_dir / \"leaves_train_FSP{}.npy\".format(num_FSPs_toData), x_train)\n",
    "        np.save(data_dir / \"is_left_arr_train_FSP{}.npy\".format(num_FSPs_toData), y_train)\n",
    "        np.save(data_dir / \"global_tag_train_FSP{}.npy\".format(num_FSPs_toData), z_train)\n",
    "\n",
    "        np.save(data_dir / \"leaves_val_FSP{}.npy\".format(num_FSPs_toData), x_val)\n",
    "        np.save(data_dir / \"is_left_arr_val_FSP{}.npy\".format(num_FSPs_toData), y_val)\n",
    "        np.save(data_dir / \"global_tag_val_FSP{}.npy\".format(num_FSPs_toData), z_val)\n",
    "\n",
    "        np.save(data_dir / \"leaves_test_FSP{}.npy\".format(num_FSPs_toData), x_test)\n",
    "        np.save(data_dir / \"is_left_arr_test_FSP{}.npy\".format(num_FSPs_toData), y_test)\n",
    "        np.save(data_dir / \"global_tag_test_FSP{}.npy\".format(num_FSPs_toData), z_test)\n",
    "\n",
    "    \n",
    "    print(\"\")\n",
    "    #del df_num_subset\n",
    "\n",
    "\n",
    "    del df_num_subset\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"saving is done\")\n",
    "now = datetime.now()\n",
    "print(\"time at end =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
